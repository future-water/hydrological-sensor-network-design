{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import fsspec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Dask cluster\n",
    "cluster = LocalCluster(\n",
    "    memory_limit='32GB',\n",
    "    n_workers=4,\n",
    "    dashboard_address=\":8787\"\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Open dataset (NWM v2.1 from S3) and subset by COMIDs and broad time range\n",
    "url = 's3://noaa-nwm-retrospective-3-0-pds/CONUS/zarr'\n",
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "outputlist = fs.ls(url)\n",
    "ds = xr.open_dataset(fs.get_mapper(outputlist[1]), engine='zarr', backend_kwargs={'consolidated':True})\n",
    "# 3) Read TX IDs\n",
    "ids_df = pd.read_csv('TX_ids.csv', low_memory=False)\n",
    "ids_list = ids_df['COMID'].tolist()\n",
    "idx = ds.feature_id.isin(ids_list)\n",
    "\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    ds_sub = ds[['streamflow']].sel(feature_id=idx)\n",
    "\n",
    "ds_sub = ds_sub.chunk({\"feature_id\": 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Dailyflow resample\n",
    "ds_daily = ds_sub.resample(time=\"D\").interpolate(\"linear\")\n",
    "\n",
    "# Convert to float32 to reduce memory usage\n",
    "ds_daily = ds_daily.astype(np.float32)\n",
    "ds_daily = ds_daily.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Loop over each year, pivot, and write CSV\n",
    "years = ds_daily.time.dt.year\n",
    "min_year = int(years.min().compute())\n",
    "max_year = int(years.max().compute())\n",
    "\n",
    "for year in range(min_year, max_year + 1):\n",
    "    ds_year = ds_daily.sel(time=str(year))\n",
    "\n",
    "    # Convert to pivoted DataFrame: time as rows, feature_id as columns\n",
    "    da_reset = ds_year[\"streamflow\"].reset_index([\"time\", \"feature_id\"])\n",
    "    df_year = da_reset.to_pandas()\n",
    "\n",
    "    # Make time the index, feature_id the columns\n",
    "    df_year.index = da_reset[\"time\"].values\n",
    "    df_year.columns = da_reset[\"feature_id\"].values\n",
    "\n",
    "    # Write to CSV\n",
    "    csv_out = f\"dailyflow_{year}.csv\"\n",
    "    df_year.to_csv(csv_out)\n",
    "    print(f\"Wrote {csv_out}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
